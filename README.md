# LLM_Improving_ASR-ConferencePaper
Our research was inspired by a challenge presented at the GenSEC Challenge, part of the IEEE Spoken Language Technology (SLT) 2024 conference, titled "Text-based Generative Speech Error Correction with LLMs." The challenge highlighted the problem of improving the accuracy of Automatic Speech Recognition (ASR) outputs by correcting errors in the n-best hypotheses generated by ASR systems. We successfully addressed this problem by developing a method that maps these n-best hypotheses to the ground truth speech transcription (H2T). Our approach leverages different Large Language Models (LLMs) and opens up new avenues for integrating second-pass LLM-based rewriting techniques within the speech recognition community.
here is link to the conference [Gensec](https://sites.google.com/view/gensec-challenge/home)

## Results 
<p align="center">
  <img src="https://github.com/user-attachments/assets/11f31f3b-fee5-4e9d-918f-d5d71a843897" width="70%" />
</p>

<div align="center" style="width: 50%;">
  <p>
    *Few-Shot Results of LLM models. Numbers shown under each category are the WER (word error rate). The acronyms represent the following models: BL for BaseLine and BL a for baseline with no space-separation, Mist for Mistral, Gem for Gemma, Lam for LLaMA, and finally Ens for ensemble. The performance metric used for comparison is WER in percentage (%). Finally, O_nb stands for Oracle n-best.* 
  </p>
</div>




As viewed from the results,  <br>
Our study showcases the effectiveness of advanced LLMs and ensemble decision fusion in enhancing ASR transcription quality. Mistral positions itself as a robust model, consistently improving transcription accuracy over baseline methods across various datasets. While LLaMa and Gemma demonstrate mixed performance individually, their integration in ensemble configurations proves beneficial, highlighting the complementary strengths of diverse LLM architectures. All models fix grammatical errors and stuttering, making them suitable for tasks where a transcription is unclean and needs a complete change. Future research should explore refining prompt-tuning techniques tailored to specific ASR challenges and expanding dataset diversity to strengthen generalization capabilities. Furthermore, integrating real-time feedback mechanisms and adapting LLMs to dynamic environments can further advance ASR technologyâ€™s reliability and applicability in practical settings. Overall, the best performance we have reached is a decrease in the WER by 62% done by the ensemble approach in the ls_clean dataset. 
<br>
**Following a detailed explanation on how we managed to reach such outstanding outcome.**


## ASR error Analysis 
A detailed analysis on what reasons might cause an ASR to make errors which was discussed in the paper and here is an overview for it 

<p align="center">
  <img src="https://github.com/user-attachments/assets/a44aa9e8-0d83-42e4-9f80-3f08220c626f" width="70%" />
</p>

<br>
Errors in ASR systems can be broadly categorized into three layers:
<br>

1. **Environmental Errors (Layer 1):** 
  - These are caused by external factors such as background noise, speaker accent, and speech rate, which the ASR system cannot control. <br> <br>
2. **Model and External Interaction Errors (Layer 2):**
  - These result from a combination of ASR model inefficiencies and external factors, such as slight mispronunciations or overlapping speech. <br> <br> 
3. **Internal Model Errors (Layer 3):**
  - These are intrinsic errors generated by the ASR system during transcription, such as misrecognition of words or phrases.

<br>
These error layers impact each other. For example, background noise (Layer 1) can lead to misheard words and context misunderstandings (Layer 2), which in turn can cause misspellings and other transcription errors (Layer 3). Figure2 depicts an illustration of various errors in each layer. The focus of our work in this paper addresses the errors in Layer 3, which are extensively discussed in literature.
<br> 


A detailed analysis will be further on supported in the github repo discussing how we managed to analyse errors more in the third layer just like we did in the methodology part in the paper but in a more detailed way. (still in the editing mode - not added yet) 


## Transcription methods
Our research has extensively examined transcription methods and their potential impact on the prompt engineering techniques we intend to utilize. We identified two primary transcription methods: 

<br>

- Literal Transcription: Captures exactly what was spoken, including any grammatical inaccuracies.
- Clean Transcription: Focuses on producing grammatically correct and semantically coherent transcriptions, even if it requires minor deviations from the original speech.

We propose a comprehensive approach that leverages the n-best hypotheses generated by ASR systems. By applying prompt-tuning techniques and ensemble methods, will be discussed later, we aim to correct and refine these hypotheses, thereby enhancing overall transcription accuracy. Our contributions include an in-depth analysis of ASR output errors and their impact on transcription quality, as well as the development of prompt-tuning strategies for LLMs to address and correct these errors.


## Used Data
We used different datasets with different descriptions for each one. <br>
|DataSet name|description|
| --- | --- |
|**LS**| LibriSpeech is a public corpus of read speech from audiobooks, including 1,000 hours of speech data with diverse speakers, genders, and accents. |
|**CHiME-4**|  includes real and simulated noisy recordings in four noisy environments, i.e., bus, cafe, pedestrian area, and street junction |
|**WSJ**| The Wall Street Journal. includes read speech from speakers in a controlled environment, with a focus on business news and financial data. |
|**SB**|  The SwitchBoard corpus is a telephone speech dataset collected from conversations between pairs of speakers. It focuses on North American English and involves over 2.4k conversations from approximately 200 speakers. |
|**CV**| CommonVoice contains speech recordings from diverse speakers in over 60 languages. |
|**Tedlium-3**| is a dataset of speech recorded from TED Talks in multiple languages. It contains a diverse range of background noise, speaker accents, speech topics, etc.  |
|**LRS2**|  Lip Reading Sentences 2. is a large-scale publicly available labeled audio-visual dataset, consisting of 224 hours of video clips from BBC programs. |
|**ATIS**|  Airline Travel Information System (ATIS) is a dataset comprising spoken queries for air travel information, such as flight times, prices, and availability. |
|**CORAAL**|  The Corpus of Regional African American Language. includes audio recordings along with the time-aligned orthographic transcription from over 150 sociolinguistic interviews. |

Here is a link to the dataset used from [hyporadise](https://huggingface.co/datasets/PeacefulData/HyPoradise-v0)
Also, here is a detailed description to the dataset in their own published [paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/6492267465a7ac507be1f9fd1174e78d-Paper-Datasets_and_Benchmarks.pdf)

## Used LLMs

We mainly used 3 LLMs which are LLaMa, Mistral and Gemma. The three were chosen based on their diversity of architecture, their availability, and they state-of-the-art in their own fields as well. Each of them has favorable reviews in the existing literature. We managed to use exactly three models based on the nature of ensemble approach which we will be using later on. 





## approaches & techniques

We tried searching different approaches and techniques to be used in each and every LLMs. At the end of the day we managed to use zero-shot prompting and few-shot prompting techniques under the category prompt engineering. Afterwards, we managed to use the ensemble technique to favor the outputs of the three models in each prompting technique and get the best out of the three which as seen from the results above decreased the overall WER. 
However, we believe in only prompt engineering is not enough to improve the performance of ASR transcription. 
